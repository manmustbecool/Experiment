{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPO2vWQ0W9nR/UVrcdyGusJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manmustbecool/Experiment/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRsd_uRKOLEa"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "# Step 1: Load the smallest LLM and tokenizer\n",
        "# Using \"bigscience/bloom-560m\" as an example of a small LLM.\n",
        "model_name = \"bigscience/bloom-560m\"\n",
        "# model_name = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Step 2: Configure PEFT with LoRA (Low-Rank Adaptation)\n",
        "# LoRA reduces the number of trainable parameters, making fine-tuning efficient.\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,  # Specify the task type (causal language modeling)\n",
        "    inference_mode=False,          # Set to False for training mode\n",
        "    r=4,                           # Rank of the LoRA matrices (smaller for efficiency)\n",
        "    lora_alpha=16,                 # Scaling factor for LoRA\n",
        "    lora_dropout=0.1               # Dropout rate for LoRA layers\n",
        ")\n",
        "model = get_peft_model(model, peft_config)  # Wrap the base model with the PEFT configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"imdb\")\n",
        "print(dataset['train'])"
      ],
      "metadata": {
        "id": "wJzEPXVSOVId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load the IMDb dataset and create a small sample\n",
        "dataset = load_dataset(\"imdb\", split=\"train\")  # Load the full training split\n",
        "subset_size = int(0.005 * len(dataset))  # Calculate 0.5% of the dataset size\n",
        "small_sample = dataset.select(range(subset_size))  # select a subset\n",
        "print(small_sample)\n",
        "\n",
        "# Step 4: Tokenize the dataset\n",
        "# Convert text data into tokenized format suitable for the model.\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=512  # Align batch size to match your training configuration\n",
        ")\n",
        "print(tokenized_dataset)\n",
        "\n",
        "print(tokenized_dataset[0])"
      ],
      "metadata": {
        "id": "LbyK0m-NOXn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Define training arguments\n",
        "# Specify hyperparameters and settings for the training process.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",          # Directory to save training results\n",
        "    # eval_strategy=\"epoch\",    # Evaluate the model at the end of each epoch\n",
        "    learning_rate=2e-5,             # Learning rate for the optimizer\n",
        "    per_device_train_batch_size=4,  # Batch size per device\n",
        "    num_train_epochs=1,             # Number of training epochs\n",
        "    weight_decay=0.01,              # Weight decay for regularization\n",
        "    save_total_limit=1,             # Limit the number of saved checkpoints\n",
        "    label_names=[\"label\"],          # Explicitly set label_names\n",
        "    report_to=\"none\"                # Disable integration with W&B\n",
        ")\n",
        "\n",
        "# Step 6: Initialize the Trainer\n",
        "# The Trainer class handles the training loop and evaluation.\n",
        "trainer = Trainer(\n",
        "    model=model,                    # Model to be trained\n",
        "    args=training_args,             # Training arguments\n",
        "    train_dataset=tokenized_dataset # Training dataset\n",
        ")\n",
        "\n",
        "# Step 7: Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Step 8: Save the fine-tuned model\n",
        "# Save the model and tokenizer for future use.\n",
        "model.save_pretrained(\"./fine_tuned_model\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
        "\n",
        "print(\"training finished\")"
      ],
      "metadata": {
        "id": "sNTzvTEzOe2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "# Sample dataset provided in the prompt\n",
        "sample_data = [\n",
        "    {\"prompt\": \"ww\", \"response\": \"ssss\"},\n",
        "    {\"prompt\": \"dd\", \"response\": \"ss\"},\n",
        "    {\"prompt\": \"ss\", \"response\": \"sss\"}\n",
        "]\n",
        "\n",
        "# Load the dataset into a Hugging Face Dataset object\n",
        "dataset = Dataset.from_list(sample_data)\n",
        "\n",
        "# Initialize the tokenizer and the small LLM (e.g., GPT-2-small)\n",
        "model_name = \"bigscience/bloom-560m\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Step 2: Configure PEFT with LoRA (Low-Rank Adaptation)\n",
        "# LoRA reduces the number of trainable parameters, making fine-tuning efficient.\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,  # Specify the task type (causal language modeling)\n",
        "    inference_mode=False,          # Set to False for training mode\n",
        "    r=4,                           # Rank of the LoRA matrices (smaller for efficiency)\n",
        "    lora_alpha=16,                 # Scaling factor for LoRA\n",
        "    lora_dropout=0.1               # Dropout rate for LoRA layers\n",
        ")\n",
        "# model = get_peft_model(model, peft_config)  # Wrap the base model with the PEFT configuration\n",
        "\n",
        "\n",
        "# Tokenize the sample dataset for fine-tuning\n",
        "def preprocess_function(example):\n",
        "    inputs = tokenizer(example[\"prompt\"], truncation=True, padding=True, max_length=64, return_tensors=\"pt\")\n",
        "    labels = tokenizer(example[\"response\"], truncation=True, padding=True, max_length=64, return_tensors=\"pt\")[\"input_ids\"]\n",
        "    return {\"input_ids\": inputs[\"input_ids\"][0], \"labels\": labels[0]}\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess_function)\n",
        "\n",
        "for i in range(len(tokenized_dataset)):\n",
        "    print(tokenized_dataset[i])\n",
        "\n",
        "# Step 5: Define training arguments\n",
        "# Specify hyperparameters and settings for the training process.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",          # Directory to save training results\n",
        "    # eval_strategy=\"epoch\",    # Evaluate the model at the end of each epoch\n",
        "    learning_rate=2e-5,             # Learning rate for the optimizer\n",
        "    per_device_train_batch_size=8,  # Batch size per device\n",
        "    num_train_epochs=1,             # Number of training epochs\n",
        "    weight_decay=0.01,              # Weight decay for regularization\n",
        "    save_total_limit=1,             # Limit the number of saved checkpoints\n",
        "    label_names=[\"labels\"],          # Explicitly set label_names\n",
        "    report_to=\"none\"                # Disable integration with W&B\n",
        ")\n",
        "\n",
        "# Step 6: Initialize the Trainer\n",
        "# The Trainer class handles the training loop and evaluation.\n",
        "trainer = Trainer(\n",
        "    model=model,                    # Model to be trained\n",
        "    args=training_args,             # Training arguments\n",
        "    train_dataset=tokenized_dataset # Training dataset\n",
        ")\n",
        "\n",
        "# Step 7: Fine-tune the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "OrgzYwMvOk0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Define training arguments\n",
        "# Specify hyperparameters and settings for the training process.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",          # Directory to save training results\n",
        "    evaluation_strategy=\"epoch\",    # Evaluate the model at the end of each epoch\n",
        "    learning_rate=2e-5,             # Learning rate for the optimizer\n",
        "    per_device_train_batch_size=8,  # Batch size per device\n",
        "    num_train_epochs=1,             # Number of training epochs\n",
        "    weight_decay=0.01,              # Weight decay for regularization\n",
        "    save_total_limit=1,             # Limit the number of saved checkpoints\n",
        ")\n",
        "\n",
        "# Step 6: Initialize the Trainer\n",
        "# The Trainer class handles the training loop and evaluation.\n",
        "trainer = Trainer(\n",
        "    model=model,                    # Model to be trained\n",
        "    args=training_args,             # Training arguments\n",
        "    train_dataset=tokenized_dataset # Training dataset\n",
        ")\n",
        "\n",
        "# Step 7: Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Step 8: Save the fine-tuned model\n",
        "# Save the model and tokenizer for future use.\n",
        "model.save_pretrained(\"./fine_tuned_model\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
        "\n",
        "# Step 9: Compare the fine-tuned model with the original model\n",
        "# Evaluate both models on the same dataset and compare their losses.\n",
        "def evaluate_model(model, tokenizer, dataset):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    for example in dataset:\n",
        "        inputs = tokenizer(example[\"text\"], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
        "        with torch.no_grad():  # Disable gradient computation for evaluation\n",
        "            outputs = model(**inputs, labels=inputs[\"input_ids\"])  # Compute loss\n",
        "        total_loss += outputs.loss.item()  # Accumulate loss\n",
        "    return total_loss / len(dataset)  # Return average loss\n",
        "\n",
        "# Load the original model for comparison\n",
        "original_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Evaluate both models\n",
        "original_loss = evaluate_model(original_model, tokenizer, tokenized_dataset)\n",
        "fine_tuned_loss = evaluate_model(model, tokenizer, tokenized_dataset)\n",
        "\n",
        "# Print the comparison results\n",
        "print(f\"Original Model Loss: {original_loss}\")\n",
        "print(f\"Fine-Tuned Model Loss: {fine_tuned_loss}\")"
      ],
      "metadata": {
        "id": "RB6u8UrGOpJH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}